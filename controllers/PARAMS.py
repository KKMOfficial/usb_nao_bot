"""set these parameters before starting the simulation"""
PROJECT_DIRECTORY = "E:\\SBU\\Semester8\\FinalProject\HIVE_MIND\\"
WEBOTS_PYTHON_BIN = "E:\\4_Installed_Softwares\\Webots\\lib\\controller\\python39"
LOAD_TARGET_ACTOR_NETWORK_FROM_DISK,TARGET_ACTOR_NETWORK_BACKUP_ADDRESS = False,"{}data\\actor_target_backup.pt".format(PROJECT_DIRECTORY)
LOAD_TARGET_CRITIC_NETWORK_FROM_DISK,TARGET_CRITIC_NETWORK_BACKUP_ADDRESS = False,"{}data\\critic_target_backup.pt".format(PROJECT_DIRECTORY)
LOAD_BUFFER_FROM_FILE,BUFFER_BACKUP_ADDRESS = False,"{}controllers\\MAIN_AGENT_SUPERVISOR\\replay_buffer[size=268101].pkl".format(PROJECT_DIRECTORY)
LOAD_HISTORY_FROM_FILE,HISTORY_BACKUP_ADDRESS = False,"{}data\\HISTORY.pkl".format(PROJECT_DIRECTORY)
TENSORBOARD_SUMMARY_FOLDER = "{}runs".format(PROJECT_DIRECTORY)
STORE_ACTOR_NETWORK_TO_FILE = True
STORE_CRITIC_NETWORK_TO_FILE = True
TRAINING_MODE = True
USE_SIMULATOR = True
ACTOR_STRUCTURE = [1024,1024,1024,1024,1024,1024,1024,1024,1024,1024,1024]
CRITIC_STRUCTURE = [1024,1024,1024,1024,1024,1024,1024,1024,1024,1024,1024]
ACTOR_NOMALIZE = True
CRITIC_NORMALIZE = True
ACTOR_DROPOUT = [0, 0, 0, 0,0, 0, 0, 0, 0, 0, 0]
CRITIC_DROPOUT = [0, 0, 0, 0,0, 0, 0, 0, 0, 0, 0]

MU_NET_STRUCTURE = [1024,1024,1024,1024,1024,1024,1024,1024,1024,1024,1024]
MU_NET_DROPOUT = [0,0,0,0,0,0,0,0,0,0,0]
SIGMA_NET_STRUCTURE = [1024,1024,1024,1024,1024]
SIGMA_NET_DROPOUT = [0,0,0,0,0,0,0,0,0,0,0]
TAU = 1e-2
MAX_BUFFER_SIZE = 1000000
ONLINE_BUFFER_SIZE = 1000000
NUM_OF_EPISODES = 100000

BEST_SAMPLES_FILE_ADDRESS = "{}data\\best_samples.pkl".format(PROJECT_DIRECTORY)
BEST_SAMPLE_THRESHOLD = 40

# SIMULATOR HYPER PARAMETERS
MIN_TRAJECTORY_STEP = 6
UPDATE_STEP = 100
UPDATE_RADIUS = 10
NUMBER_OF_RECOVERIES = lambda depth,period : 5 if depth<10 else 10 if depth<20 else 20 if depth<30 else 40
# lambda depth,period : 1+int(depth/5)+5**(int(depth/10)%period)

MIN_Z_FALL = 0.33
MAX_TORSO_ANGLE = 0.2
MIN_TORSO_ANGLE = -0.25
MIN_DISTANCE = 0.05

MAX_INIT_GOAL_RADIUS = 1.02
NETWORK_INITIAL_TRAINING = False
OFFLINE_ITERATIONS = 20
OFFLINE_EPISODES = 50
OFFLINE_SUBSET = 200


TRAIN_LOG_START = 0
SIMULATOR_LOG_START = 0

# PPO HYPER PARAMETERS  
CLIP_RATIO = 2e-2
GAMMA = 0.99
ACTOR_LEARNING_RATE = 1e-2
CRITIC_LEARNING_RATE = 1e-2
BETA = 0.0

# ROBOT HYPER PARAMETERS
LOWER_MOTOR_MULTIPLIER = 1
UPPER_MOTOR_MULTIPLIER = 1
EPISODE_INTERVAL = 1
WORKER_EPISODES = 200




# GAMMA = 0.98 -> effective rewards means : consider next 50 states in the estimation of the state value
EFFECTIVE_REWARDS = int(1/(1-GAMMA))
BACKUP_CHECK_POINTES = list(range(0,NUM_OF_EPISODES+1,EPISODE_INTERVAL))
ACTOR_MAX_OUTPUT = 1.0
ACTOR_MIN_OUTPUT = -1.0
DECAY_PERIOD = 100000
ARENA_WIDTH = 1000
ARENA_HEIGHT = 1000
INIT_Z = 0.332209
COM_BALANCE_Z = 0.334
MAX_Z_CHANGE = 0.1
BATCH_SIZE = 64
HORIZON = 10000
INIT_PORT = 550
NOISE_CLIP = 0.5
POLICY_NOISE = 0.2
POLICY_FREQUENCY = 2
WORKER_CONTROLLER_PATH = 'E:\\SBU\\Semester8\\FinalProject\\HIVE_MIND\\controllers\\WORKER_AGENT_SUPERVISOR\\WORKER_AGENT_SUPERVISOR.py'
PYTHON_PATH = 'E:\\4_Installed_Softwares\\Python3.9\\python.exe'
MAIN_AGENT_AS_SERVER = False
MAIN_AGENT_PORT = 500
ALGORITHM = 'PPOV1'
ALGO_DETEMINISTIC = False
WORKER_RANDOM_INITIALIZATION = False
WORKERS = {
    'NAO1':'http://localhost:550',
    'NAO2':'http://localhost:551',
    'NAO3':'http://localhost:552',
    'NAO4':'http://localhost:553',
    'NAO5':'http://localhost:554',
    'NAO6':'http://localhost:555',
    'NAO7':'http://localhost:556',
    'NAO8':'http://localhost:557',
    'NAO9':'http://localhost:558',
    'NAO10':'http://localhost:559',

}
    

NOISE_LIST = [
    {'mu':0.0,'theta':0.4,'sigma':0.115},
    {'mu':0.0,'theta':1.5,'sigma':0.25},
]
